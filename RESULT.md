## Dec 1st: Experiment Configuration (results/baseline/config.yaml)

|                   | loss          | accuracy      | precision     | recall        | f1score       | iou           |
|:------------------|:--------------|:--------------|:--------------|:--------------|:--------------|:--------------|
| ('GBM', 'test')   | 0.394 (0.097) | 0.840 (0.049) | 0.806 (0.097) | 0.606 (0.026) | 0.624 (0.044) | 0.520 (0.041) |
| ('GBM', 'train')  | 0.241 (0.059) | 0.917 (0.027) | 0.933 (0.030) | 0.795 (0.043) | 0.842 (0.040) | 0.744 (0.052) |
| ('LSTM', 'test')  | 0.604 (0.047) | 0.732 (0.021) | 0.586 (0.042) | 0.661 (0.042) | 0.583 (0.053) | 0.458 (0.043) |
| ('LSTM', 'train') | 0.564 (0.016) | 0.749 (0.025) | 0.608 (0.015) | 0.702 (0.018) | 0.615 (0.022) | 0.484 (0.023) |
| ('MLP', 'test')   | 0.558 (0.220) | 0.775 (0.074) | 0.595 (0.100) | 0.606 (0.092) | 0.600 (0.096) | 0.487 (0.070) |
| ('MLP', 'train')  | 0.280 (0.100) | 0.895 (0.050) | 0.822 (0.085) | 0.823 (0.064) | 0.817 (0.073) | 0.714 (0.099) |


## Dec 2nd: Experiment Configuration (src/experiments/baseline-dec03.yaml)

实验设置: 详细配置请参考[baseline-dec03](./src/experiments/baseline-dec03.yaml)文件, 目前是一个简单的`detection task``: 先统计日平均(生理, 行动)数据, 然后给定7天的数据, 让算法判断这7天里是否发生了至少一次**Agitation**行为. 这算是一个简单的探测任务. 这里的结果对比上一个表格有一些出入是因为我在内层调参选出最优模型后, 在最终评估前用整个train+valid数据对模型进行了再拟合(refit)

算法训练: 

|                   | rocauc       | loss         | accuracy     | precision    | recall       | f1score      | iou          |
|:------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|
| ('GBM', 'test')   | 0.809(0.080) | 0.578(0.085) | 0.824(0.098) | 0.725(0.106) | 0.756(0.114) | 0.736(0.110) | 0.615(0.134) |
| ('GBM', 'train')  | 0.976(0.020) | 0.288(0.083) | 0.916(0.040) | 0.849(0.048) | 0.931(0.044) | 0.879(0.047) | 0.794(0.075) |
| ('LSTM', 'test')  | 0.754(0.098) | 1.395(0.669) | 0.797(0.100) | 0.684(0.100) | 0.683(0.086) | 0.681(0.092) | 0.554(0.107) |
| ('LSTM', 'train') | 0.930(0.030) | 0.334(0.098) | 0.850(0.043) | 0.772(0.035) | 0.865(0.049) | 0.799(0.042) | 0.679(0.057) |
| ('MLP', 'test')   | 0.748(0.076) | 1.053(0.330) | 0.818(0.073) | 0.707(0.074) | 0.618(0.026) | 0.637(0.040) | 0.522(0.050) |
| ('MLP', 'train')  | 0.928(0.046) | 0.390(0.106) | 0.901(0.031) | 0.829(0.053) | 0.828(0.041) | 0.826(0.040) | 0.722(0.053) |


貌似如果用同样的数据源, 深度学习模型很难打败简单的机器学习模型, 虽然这里我还没有fine tune...下一步可以考虑如何利用深度学习模型处理复杂数据的优势, 从数据来源和数据细粒度两个角度给分类任务增加难度


## Dec 3rd: Experiment Configuration (src/experiments/baseline-dec04.yaml)

进行一次复杂的LSTM调参, 基本把模型相关的参数(除数据外)调了个遍, 明天起来看看结果能不能打败不调参的MLP和GBM模型

这里我进行了两次实验, 一次是外层(train-test)使用time-split, 内层(train-valid)使用patient-split, 效果不太好, 对比上一个结果, finetuned-LSTM表现甚至不如不调参的LSTM, 可以看到模型不仅存在严重的过拟合, 连训练指标也比不上GBM

|                   | rocauc       | loss         | accuracy     | precision    | recall       | f1score      | iou          |
|:------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|
| ('GBM', 'test')   | 0.811(0.081) | 0.576(0.086) | 0.824(0.098) | 0.725(0.106) | 0.756(0.114) | 0.736(0.110) | 0.615(0.134) |
| ('GBM', 'train')  | 0.971(0.021) | 0.309(0.067) | 0.896(0.038) | 0.833(0.044) | 0.916(0.040) | 0.861(0.046) | 0.765(0.067) |
| ('LSTM', 'test')  | 0.724(0.088) | 2.288(0.513) | 0.796(0.087) | 0.694(0.150) | 0.581(0.026) | 0.590(0.030) | 0.482(0.040) |
| ('LSTM', 'train') | 0.941(0.034) | 0.240(0.095) | 0.898(0.044) | 0.860(0.049) | 0.823(0.095) | 0.836(0.074) | 0.738(0.110) |
| ('MLP', 'test')   | 0.748(0.076) | 1.053(0.330) | 0.818(0.073) | 0.707(0.074) | 0.618(0.026) | 0.637(0.040) | 0.522(0.050) |
| ('MLP', 'train')  | 0.928(0.046) | 0.390(0.106) | 0.901(0.031) | 0.829(0.053) | 0.828(0.041) | 0.826(0.040) | 0.722(0.053) |

然后我将内层(train-valid)给换成了和外层(train-test)一致的time-split, 这种情况下, 训练指标确实提升了不少, 但过拟合更加严重了, 我不知道创新点是否可以围绕这方面展开, 即一个技术贡献就是如何在小样本里防止过拟合风险, 当然这里测试集数据量小也是一大痛点, 另外就是这里输入深度学习模型的特征已经不是原始数值而是统计指标了, 深度学习二创的能力不是很强....

|                   | rocauc       | loss         | accuracy     | precision    | recall       | f1score      | iou          |
|:------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|
| ('GBM', 'test')   | 0.810(0.079) | 0.577(0.084) | 0.824(0.098) | 0.725(0.106) | 0.756(0.114) | 0.736(0.110) | 0.615(0.134) |
| ('GBM', 'train')  | 0.959(0.025) | 0.339(0.067) | 0.873(0.031) | 0.790(0.024) | 0.897(0.040) | 0.823(0.031) | 0.711(0.044) |
| ('LSTM', 'test')  | 0.775(0.116) | 2.210(1.040) | 0.805(0.081) | 0.678(0.062) | 0.690(0.125) | 0.675(0.094) | 0.553(0.106) |
| ('LSTM', 'train') | 0.997(0.003) | 0.040(0.037) | 0.988(0.013) | 0.979(0.022) | 0.980(0.022) | 0.980(0.022) | 0.962(0.041) |
| ('MLP', 'test')   | 0.728(0.067) | 1.143(0.281) | 0.799(0.090) | 0.718(0.152) | 0.593(0.030) | 0.604(0.029) | 0.493(0.038) |
| ('MLP', 'train')  | 0.926(0.051) | 0.376(0.135) | 0.899(0.053) | 0.838(0.074) | 0.828(0.086) | 0.832(0.080) | 0.733(0.110) |

另外有考虑增加batch-size, 让模型先遍历更丰富的信息再迭代自己, 发现这样处理并不能带来多少收益. 目前还是优先考虑任务设计, 以及深度学习数据的再处理. 另外附上LSTM调参的结果(效果前10和倒数前10的模型), 可能参考价值有限, 后续调参的时候尽量使用简单的模型 (lr=1e-3, dropout=0.2~0.4, hidden_size=64, num_layers=2, patience=10~20)

比较一下lstm和lstm(with train-valid retrain), 

|                         | rocauc       | loss         | accuracy     | precision    | recall       | f1score      | iou          |
|:------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|
| ('GBM', 'test')         | 0.810(0.079) | 0.577(0.084) | 0.824(0.098) | 0.725(0.106) | 0.756(0.114) | 0.736(0.110) | 0.615(0.134) |
| ('GBM', 'valid')        | 0.797(0.067) | 0.588(0.137) | 0.739(0.107) | 0.666(0.079) | 0.706(0.100) | 0.652(0.100) | 0.516(0.097) |
| ('LSTM', 'test')        | 0.741(0.112) | 1.461(0.636) | 0.787(0.107) | 0.691(0.111) | 0.734(0.130) | 0.700(0.119) | 0.573(0.149) |
| ('LSTM', 'valid')       | 0.782(0.104) | 0.920(0.162) | 0.757(0.060) | 0.632(0.063) | 0.708(0.092) | 0.639(0.076) | 0.507(0.069) |
| ('LSTM_refit', 'test')  | 0.736(0.114) | 1.669(0.711) | 0.766(0.107) | 0.636(0.099) | 0.660(0.136) | 0.640(0.114) | 0.515(0.125) |
| ('LSTM_refit', 'valid') | 0.788(0.055) | 1.077(0.269) | 0.772(0.063) | 0.649(0.052) | 0.708(0.033) | 0.657(0.048) | 0.525(0.038) |

事实证明在这个数据集上, train-valid retrain能提升机器学习算法, 但并不能提升LSTM模型的效果(我觉得test表现比valid高也是挺离谱的)