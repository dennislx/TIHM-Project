## Dec 1st: Experiment Configuration (results/baseline/config.yaml)

|                   | loss          | accuracy      | precision     | recall        | f1score       | iou           |
|:------------------|:--------------|:--------------|:--------------|:--------------|:--------------|:--------------|
| ('GBM', 'test')   | 0.394 (0.097) | 0.840 (0.049) | 0.806 (0.097) | 0.606 (0.026) | 0.624 (0.044) | 0.520 (0.041) |
| ('GBM', 'train')  | 0.241 (0.059) | 0.917 (0.027) | 0.933 (0.030) | 0.795 (0.043) | 0.842 (0.040) | 0.744 (0.052) |
| ('GBM', 'valid')  | 0.367 (0.062) | 0.827 (0.044) | 0.697 (0.095) | 0.619 (0.019) | 0.631 (0.016) | 0.520 (0.020) |
| ('LSTM', 'test')  | 0.604 (0.047) | 0.732 (0.021) | 0.586 (0.042) | 0.661 (0.042) | 0.583 (0.053) | 0.458 (0.043) |
| ('LSTM', 'train') | 0.564 (0.016) | 0.749 (0.025) | 0.608 (0.015) | 0.702 (0.018) | 0.615 (0.022) | 0.484 (0.023) |
| ('LSTM', 'valid') | 0.601 (0.033) | 0.665 (0.066) | 0.568 (0.030) | 0.665 (0.044) | 0.534 (0.053) | 0.406 (0.047) |
| ('MLP', 'test')   | 0.558 (0.220) | 0.775 (0.074) | 0.595 (0.100) | 0.606 (0.092) | 0.600 (0.096) | 0.487 (0.070) |
| ('MLP', 'train')  | 0.280 (0.100) | 0.895 (0.050) | 0.822 (0.085) | 0.823 (0.064) | 0.817 (0.073) | 0.714 (0.099) |
| ('MLP', 'valid')  | 0.668 (0.213) | 0.700 (0.084) | 0.659 (0.030) | 0.692 (0.050) | 0.639 (0.048) | 0.489 (0.059) |


## Dec 3rd: Experiment Configuration (src/experiments/baseline-dec03.yaml)

实验设置: 详细配置请参考`yaml`文件, 目前是一个简单的detection task: 先统计日平均(生理, 行动)数据, 然后给定7天的数据, 让算法判断这7天里是否发生了至少一次**Agitation**行为. 这算是一个简单的探测任务

|                   | rocauc       | loss         | accuracy     | precision    | recall       | f1score      | iou          |
|:------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|
| ('GBM', 'test')   | 0.810(0.080) | 0.577(0.084) | 0.824(0.098) | 0.725(0.106) | 0.756(0.114) | 0.736(0.110) | 0.615(0.134) |
| ('GBM', 'train')  | 0.972(0.019) | 0.309(0.050) | 0.893(0.035) | 0.814(0.040) | 0.921(0.032) | 0.849(0.043) | 0.748(0.062) |
| ('LSTM', 'test')  | 0.751(0.086) | 1.589(0.533) | 0.783(0.092) | 0.670(0.100) | 0.656(0.071) | 0.652(0.061) | 0.524(0.075) |
| ('LSTM', 'train') | 0.939(0.029) | 0.315(0.086) | 0.848(0.049) | 0.774(0.057) | 0.863(0.044) | 0.798(0.060) | 0.679(0.082) |
| ('MLP', 'test')   | 0.753(0.104) | 0.952(0.325) | 0.835(0.080) | 0.770(0.132) | 0.649(0.043) | 0.679(0.063) | 0.560(0.072) |
| ('MLP', 'train')  | 0.940(0.019) | 0.378(0.066) | 0.909(0.022) | 0.866(0.041) | 0.843(0.035) | 0.852(0.032) | 0.755(0.045) |


貌似如果用同样的数据源, 深度学习模型很难打败简单的机器学习模型, 虽然这里我还没有fine tune...下一步可以考虑如何利用深度学习模型处理复杂数据的优势, 从数据来源和数据细粒度两个角度给分类任务增加难度